library(splines)
#setwd('C:\\Users\\user\\Documents\\Database')
#setwd('/Users/vanshajchadha/Documents/Database/')
setwd('/Users/edisonlo/Desktop/Stat_project/Beijing-PM2.5Data')
data <- read.csv('DataCleaned.csv')
head(data)
#Replacing the missing values with mean of the column
data$pm2.5[is.na(data$pm2.5)] = mean(data$pm2.5, na.rm = TRUE)
#Rounding the value to 2 decimal places
data$pm2.5 = round(data$pm2.5, 2)
head(data)
#Log scale
skew(data$pm2.5)
pm.log <-log(data$pm2.5) #pm 2.5 is highly right skewed.
hist(pm.log)
#Data cleaning
attributes <- data[-c(1:24), 6:13]
library(gam)
library(psych)
library(GPArotation)
library(splines)
#setwd('C:\\Users\\user\\Documents\\Database')
#setwd('/Users/vanshajchadha/Documents/Database/')
setwd('/Users/edisonlo/Desktop/Stat_project/')
data <- read.csv('BeijingPM25.csv')
library(gam)
library(psych)
library(GPArotation)
library(splines)
#setwd('C:\\Users\\user\\Documents\\Database')
#setwd('/Users/vanshajchadha/Documents/Database/')
setwd('/Users/edisonlo/Desktop/Stat_project/')
data <- read.csv('BeijingPM25.csv')
library(gam)
library(psych)
library(GPArotation)
library(splines)
#setwd('C:\\Users\\user\\Documents\\Database')
#setwd('/Users/vanshajchadha/Documents/Database/')
setwd('/Users/edisonlo/Desktop/Stat_project/')
data <- read.csv('BeijingPM25.csv')
library(gam)
library(psych)
library(GPArotation)
library(splines)
#setwd('C:\\Users\\user\\Documents\\Database')
#setwd('/Users/vanshajchadha/Documents/Database/')
setwd('/Users/edisonlo/Desktop/Stat_project/Beijing-PM2.5Data/')
data <- read.csv('BeijingPM25.csv')
library(gam)
library(psych)
library(GPArotation)
library(splines)
#setwd('C:\\Users\\user\\Documents\\Database')
#setwd('/Users/vanshajchadha/Documents/Database/')
setwd('/Users/edisonlo/Desktop/Stat_project/Beijing-PM2.5Data/')
data <- read.csv('BeijingPM25.csv')
library(gam)
library(psych)
library(GPArotation)
library(splines)
#setwd('C:\\Users\\user\\Documents\\Database')
#setwd('/Users/vanshajchadha/Documents/Database/')
setwd('/Users/edisonlo/Desktop/Stat_project/Beijing-PM2.5Data/')
data <- read.csv('BeijingPM25.csv')
library(gam)
library(psych)
library(GPArotation)
library(splines)
#setwd('C:\\Users\\user\\Documents\\Database')
#setwd('/Users/vanshajchadha/Documents/Database/')
setwd('/Users/edisonlo/Desktop/Stat_project/Beijing-PM2.5Data/')
data <- read.csv('BeijingPM25.csv')
head(data)
#Replacing the missing values with mean of the column
data$pm2.5[is.na(data$pm2.5)] = mean(data$pm2.5, na.rm = TRUE)
#Rounding the value to 2 decimal places
data$pm2.5 = round(data$pm2.5, 2)
head(data)
#Log scale
skew(data$pm2.5)
pm.log <-log(data$pm2.5) #pm 2.5 is highly right skewed.
hist(pm.log)
#Data cleaning
attributes <- data[-c(1:24), 6:13]
attributes <- attributes[, -5]
#Correlation matrix
cor(attributes) #Obesrvation: DEWP, TEMP, PRES are highly correlated
#Dimension reduction: Factor analysis
fit<-principal(r=attributes, nfactors=6,rotate="varimax")
fit #Oservation: Factor analysis result verifies that we can create new variable from DEWP, TEMP and PRES without losing many information.
#Linear model on all variables
fit <- lm(data$pm2.5~data$Is+data$Ir+data$DEWP+data$TEMP+data$cbwd+data$Iws)
summary(fit) #All variables are significant under t test. Still, we need to reduce the dimension and re-do this part.
plot(fit$residuals, main='Residual plots', ylab='Residuals', pch='.', col='blue') #Residuals are also skewd. We may consider a log model if we need linear model for inference
#nonlinear model on all variables: exists
fit2 <- gam(data$pm2.5~s(data$Is, 1)+ s(data$Ir, 1)+ s(data$DEWP, 1)+ s(data$TEMP, 2)+  s(data$Iws, 1))
summary(fit2)
fit3 <- gam(data$pm2.5~s(data$Is, 3)+ s(data$Ir, 3)+ s(data$DEWP, 3)+ s(data$TEMP, 2)+  s(data$Iws, 3))
summary(fit3)
fit4 <- gam(data$pm2.5~s(data$Is, 3)+ s(data$Ir, 2)+ s(data$DEWP, 2)+ s(data$TEMP, 2)+  s(data$Iws, 2))
summary(fit4)
#Both linear and nonlinear models are significant under nonparametric table of F test. We need to pick one to forecast by cross validation.
#Time series plot
time.pm <- 1:length(data$pm2.5)
plot(time.pm, data$pm2.5, type='l', col='grey')
#Local regression for different spans
for (i in c(0.1, 0.3, 0.5)){
fit.lr <- loess(data$pm2.5~time.pm, span=i)
lines(fit.lr$fitted, type='l', col=runif(3, 10, 100))
}
#Cubic spline
fit.sp3 <- lm(data$pm2.5~bs(time.pm, knots=seq(0, length(data$pm2.5), 3000)))
plot(time.pm, data$pm2.5, type='l', col='grey')
lines(fit.sp3$fitted.values, type='l', col='red')
# Remark on data set attributes:
# No: row number
# year: year of data in this row
# month: month of data in this row
# day: day of data in this row
# hour: hour of data in this row
# pm2.5: PM2.5 concentration (ug/m^3)
# DEWP: Dew Point (a?????)
# TEMP: Temperature (a?????)
# PRES: Pressure (hPa)
# cbwd: Combined wind direction
# Iws: Cumulated wind speed (m/s)
# Is: Cumulated hours of snow
# Ir: Cumulated hours of rain
library(gam)
library(psych)
library(GPArotation)
library(splines)
#setwd('C:\\Users\\user\\Documents\\Database')
#setwd('/Users/vanshajchadha/Documents/Database/')
setwd('/Users/edisonlo/Desktop/Stat_project/Beijing-PM2.5Data/')
data <- read.csv('DataCleaned.csv')
head(data)
#Replacing the missing values with mean of the column
data$pm2.5[is.na(data$pm2.5)] = mean(data$pm2.5, na.rm = TRUE)
#Rounding the value to 2 decimal places
data$pm2.5 = round(data$pm2.5, 2)
head(data)
#Log scale
skew(data$pm2.5)
pm.log <-log(data$pm2.5) #pm 2.5 is highly right skewed.
hist(pm.log)
#Data cleaning
attributes <- data[-c(1:24), 6:13]
library(gam)
library(psych)
library(GPArotation)
library(splines)
#setwd('C:\\Users\\user\\Documents\\Database')
#setwd('/Users/vanshajchadha/Documents/Database/')
setwd('/Users/edisonlo/Desktop/Stat_project/Beijing-PM2.5Data/')
data <- read.csv('BeijingPM25.csv')
head(data)
#Replacing the missing values with mean of the column
data$pm2.5[is.na(data$pm2.5)] = mean(data$pm2.5, na.rm = TRUE)
#Rounding the value to 2 decimal places
data$pm2.5 = round(data$pm2.5, 2)
head(data)
#Log scale
skew(data$pm2.5)
pm.log <-log(data$pm2.5) #pm 2.5 is highly right skewed.
hist(pm.log)
#Data cleaning
attributes <- data[-c(1:24), 6:13]
attributes <- attributes[, -5]
#Correlation matrix
cor(attributes) #Obesrvation: DEWP, TEMP, PRES are highly correlated
#Dimension reduction: Factor analysis
fit<-principal(r=attributes, nfactors=6,rotate="varimax")
fit #Oservation: Factor analysis result verifies that we can create new variable from DEWP, TEMP and PRES without losing many information.
#Linear model on all variables
fit <- lm(data$pm2.5~data$Is+data$Ir+data$DEWP+data$TEMP+data$cbwd+data$Iws)
summary(fit) #All variables are significant under t test. Still, we need to reduce the dimension and re-do this part.
plot(fit$residuals, main='Residual plots', ylab='Residuals', pch='.', col='blue') #Residuals are also skewd. We may consider a log model if we need linear model for inference
#nonlinear model on all variables: exists
fit2 <- gam(data$pm2.5~s(data$Is, 1)+ s(data$Ir, 1)+ s(data$DEWP, 1)+ s(data$TEMP, 2)+  s(data$Iws, 1))
summary(fit2)
fit3 <- gam(data$pm2.5~s(data$Is, 3)+ s(data$Ir, 3)+ s(data$DEWP, 3)+ s(data$TEMP, 2)+  s(data$Iws, 3))
summary(fit3)
fit4 <- gam(data$pm2.5~s(data$Is, 3)+ s(data$Ir, 2)+ s(data$DEWP, 2)+ s(data$TEMP, 2)+  s(data$Iws, 2))
summary(fit4)
#Both linear and nonlinear models are significant under nonparametric table of F test. We need to pick one to forecast by cross validation.
#Time series plot
time.pm <- 1:length(data$pm2.5)
plot(time.pm, data$pm2.5, type='l', col='grey')
#Local regression for different spans
for (i in c(0.1, 0.3, 0.5)){
fit.lr <- loess(data$pm2.5~time.pm, span=i)
lines(fit.lr$fitted, type='l', col=runif(3, 10, 100))
}
#Cubic spline
fit.sp3 <- lm(data$pm2.5~bs(time.pm, knots=seq(0, length(data$pm2.5), 3000)))
plot(time.pm, data$pm2.5, type='l', col='grey')
lines(fit.sp3$fitted.values, type='l', col='red')
# Remark on data set attributes:
# No: row number
# year: year of data in this row
# month: month of data in this row
# day: day of data in this row
# hour: hour of data in this row
# pm2.5: PM2.5 concentration (ug/m^3)
# DEWP: Dew Point (a?????)
# TEMP: Temperature (a?????)
# PRES: Pressure (hPa)
# cbwd: Combined wind direction
# Iws: Cumulated wind speed (m/s)
# Is: Cumulated hours of snow
# Ir: Cumulated hours of rain
library(mice)
#setwd('C:\\Users\\user\\Documents\\Database') #Change your path here
setwd('/Users/edisonlo/Desktop/Stat_project/Beijing-PM2.5Data/')
data <- read.csv('BeijingPM25.csv')
head(data)
md.pattern(data)
#Modification on missing data code
#Replace missing value with the mean of pm2.5 in the corresponding year
dataCleaned <- data
head(dataCleaned[is.na(data$pm2.5), ])
for (i in 2010:2014){
data.mean <- mean(dataCleaned[dataCleaned$year==i, ]$pm2.5, na.rm=T)
dataCleaned[is.na(data$pm2.5) & dataCleaned$year==i, ]$pm2.5 <- data.mean
}
md.pattern(dataCleaned)
#Correlation matrix of continuous variables (ContVar)
dataContVar <- data[, c(7:9, 11:13)]
cor(dataContVar)
#Interpretation DEWP, TEMP and PRES are highly correlated.
#PCA dimension reduction
fit <- prcomp(dataContVar, scale. = T)
summary(fit) #4 pc solution, retain ~94% information
fit$rotation
#PC1: positively related to DEWP, TEMP but negative related to PRES
#PC2: negative related to Is and Ir
#PC3: negative related to Is but positively related to Ir
#Pc4: Negatively related to Iws
head(fit$x)
dataPCA <- data.frame(dataCleaned[, c(2:6, 10)], fit$x[,1:4])
head(dataPCA)
write.csv(dataPCA, file="DataCleaned.csv", row.names=FALSE)
dataPCA$cbwd <- factor(dataPCA$cbwd, levels=c('cv', 'NE', 'NW', 'SE'))
fitlm <- lm(formula = pm2.5 ~ PC1+PC2+PC3+PC4+cbwd, data=dataPCA)
summary(fitlm)
summary(fit)
?prcomp
fit$rotation
summary(fit)
fit$rotation
head(fit$x)
head(dataPCA)
?head
summary(fitlm)
?lm
knitr::opts_chunk$set(echo = TRUE)
importance(randomForest.carseats)
knitr::opts_chunk$set(echo = TRUE)
#(a)
set.seed(3356)
x <- rnorm(100)
noise <- rnorm(100)
#(b)
y <- 1 + 3*x + 5*x^2 + 10*x^3 + noise
#(c)
library(ISLR)
library(leaps)
data.full <- data.frame(x=x, y=y)
regfit.full <- regsubsets(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10), data = data.full, nvmax = 10)
reg.summary <- summary(regfit.full)
par(mfrow = c(2, 2))
#look for cp
plot(reg.summary$cp, xlab = "# variables", ylab = "Cp", type = "l")
cp.min = min(reg.summary$cp)
cp.min
points(which.min(reg.summary$cp),reg.summary$cp[which.min(reg.summary$cp)], pch=10, col="blue")
#look for bic
plot(reg.summary$bic, xlab = "# variables", ylab = "BIC", type = "l")
bic.min = min(reg.summary$bic)
bic.min
points(which.min(reg.summary$bic),reg.summary$bic[which.min(reg.summary$bic)], pch=10, col="blue")
#look for adjusted r square
plot(reg.summary$adjr2, xlab = "# variables", ylab = "Adjusted R square", type = "l")
adjr2.max = max(reg.summary$adjr2)
adjr2.max
points(which.max(reg.summary$adjr2),reg.summary$adjr2[which.max(reg.summary$adjr2)], pch=10, col="blue")
#So for best model for Cp is 5-variables model, the best for bic is also 5-variables model and for adjusted r square, the best is also 5-variables model.
#(d)
#step forward
fit.forward=regsubsets(y~x+I(x^2)+I(x^3)+I(x^4)+I(x^5)+I(x^6)+I(x^7)+I(x^8)+I(x^9)+I(x^10),
data = data.frame(x=x,y=y),nvmax=10, method="forward")
fit.summary = summary(fit.forward)
par(mfrow=c(2,2))
# look for cp
plot(fit.summary$cp, xlab="# Predictors", ylab="Cp", type="l")
cp.min = min(fit.summary$cp)
points(which.min(fit.summary$cp),fit.summary$cp[which.min(fit.summary$cp)], pch=10, col="blue")
# look for bic
plot(fit.summary$bic, xlab="# Predictors", ylab="BIC", type="l")
bic.min = min(fit.summary$bic)
points(which.min(fit.summary$bic),fit.summary$bic[which.min(fit.summary$bic)], pch=10, col="blue")
# look for adjusted r suqare
plot(fit.summary$adjr2,xlab="# Predictors", ylab="Adjusted R Square", type="l")
adjr2.max = max(fit.summary$adjr2)
adjr2.max
points(which.max(fit.summary$adjr2),fit.summary$adjr2[which.max(fit.summary$adjr2)], pch=10, col="blue")
#So for best model for Cp is 6-variables model, the best for bic is 3-variables model and for adjusted r square, the best is 6-variables model.
# step backward
fit.backward = regsubsets(y~x+I(x^2)+I(x^3)+I(x^4)+I(x^5)+I(x^6)+I(x^7)+I(x^8)+I(x^9)+I(x^10),
data = data.frame(x=x,y=y),nvmax=10, method="backward")
backward.summary = summary(fit.backward)
par(mfrow = c(2,2))
# look for cp
plot(backward.summary$cp, xlab="# Predictors", ylab="Cp", type="l")
cp.min = min(backward.summary$cp)
cp.min
points(which.min(backward.summary$cp),backward.summary$cp[which.min(backward.summary$cp)], pch=10, col="blue")
# look for bic
bic.min = min(backward.summary$bic)
bic.min
plot(backward.summary$bic, xlab="# Predictors", ylab="BIC", type="l")
points(which.min(backward.summary$bic),backward.summary$bic[which.min(backward.summary$bic)], pch=10, col="blue")
# look for adj
adjr2.max = max(backward.summary$adjr2)
adjr2.max
plot(backward.summary$adjr2,xlab="# Predictors", ylab ="Adjusted R Square", type="l")
points(which.max(backward.summary$adjr2),backward.summary$adjr2[which.min(backward.summary$adjr2)], pch=10, col="blue")
#So for best model for Cp is 3-variables model, the best for bic is 5-variables model and for adjusted r square, the best is also 5-variables model.
#(e)
library(glmnet)
set.seed(3356)
X=cbind(x,x^2,x^3,x^4,x^5,x^6,x^7,x^8,x^9,x^10)
Y=y
# Cross validation
lasso.cv = cv.glmnet(X,Y, alpha=1)
lasso.cv$lambda.1se
lasso.cv$lambda.min
plot(lasso.cv)
# using new lamda
lasso.mod=glmnet(X,Y,alpha=1, lambda=lasso.cv$lambda.min)
coef(lasso.mod)[,1]
lasso.cv$lambda.min
lasso.cv$lambda.1se
#The coefficent:
coef(lasso.mod)[,1]
#Choosing 5, or 8 or 6-variables are achieving zero MSE and the MSE is growing when the number of variables are decreasing from 6 to 1
#(f)
reg.summary
#use my own custom number
Y <- 1 + 7 * x^7 + noise
data.full <- data.frame(y = Y, x = X)
regfit.full <- regsubsets(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10), data = data.full, nvmax = 10)
reg.summary <- summary(regfit.full)
par(mfrow = c(2, 2))
plot(reg.summary$cp, xlab = "# variables", ylab = "Cp", type = "l")
points(which.min(reg.summary$cp), reg.summary$cp[which.min(reg.summary$cp)], col = "blue", pch = 10)
plot(reg.summary$bic, xlab = "# variables", ylab = "BIC", type = "l")
points(which.min(reg.summary$bic), reg.summary$bic[which.min(reg.summary$bic)], col = "blue",  pch = 10)
plot(reg.summary$adjr2, xlab = "# variables", ylab = "Adjusted r square", type = "l")
points(which.max(reg.summary$adjr2), reg.summary$adjr2[which.max(reg.summary$adjr2)], col = "blue", pch = 10)
#For best subset selection, All Cp, BIC and adjusted R square pick 5-variables model
coef(regfit.full, 5)
lasso.mat <- model.matrix(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10), data = data.full)[, -1]
lasso.cv <- cv.glmnet(lasso.mat, y, alpha = 1)
lam.min <- lasso.cv$lambda.min
lam.min
fit.lasso <- glmnet(lasso.mat, y, alpha = 1)
predict(fit.lasso, s = lam.min, type = "coefficients")[1:11, ]
#For lasso, it picks for number of variable larger than 5
#(a)
library(tree)
library(ISLR)
library(randomForest)
set.seed(3356)
train <- sample(1:nrow(Carseats), nrow(Carseats) / 2)
Carseats.train <- Carseats[train, ]
Carseats.test <- Carseats[-train, ]
#(b)
tree.carseats <- tree(Sales ~ ., data = Carseats.train)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats, pretty = 0)
y_pred <- predict(tree.carseats, newdata = Carseats.test)
mean((y_pred - Carseats.test$Sales)^2)
#Test MSE is around 4.5
#(c)
cv.carseats <- cv.tree(tree.carseats)
plot(cv.carseats$size, cv.carseats$dev, type = "b")
tree.min <- which.min(cv.carseats$dev)
points(tree.min, cv.carseats$dev[tree.min], col = "blue", cex = 2, pch = 10)
# Cross validation selected tree of size 2
# do pruning
prune.carseats <- prune.tree(tree.carseats, best = 2)
plot(prune.carseats)
text(prune.carseats, pretty = 0)
y_pred <- predict(prune.carseats, newdata = Carseats.test)
mean((y_pred - Carseats.test$Sales)^2)
#No it doesnt imporeve MSE, as it has been increaed to 6.05
#(d)
bag.carseats <- randomForest(Sales ~ ., data = Carseats.train, mtry = 10, ntree =100, importance = TRUE)
y_pred.bag <- predict(bag.carseats, newdata = Carseats.test)
mean((y_pred.bag - Carseats.test$Sales)^2)
#Test MSE is 2.6683
importance(bag.carseats)
#So the Price and Shelveloc are the most important variables
#(e)
randomForest.carseats <- randomForest(Sales ~ ., data = Carseats.train, mtry = 3, ntree = 100, importance = TRUE)
y_pref.rf <- predict(randomForest.carseats, newdata = Carseats.test)
mean((y_pref.rf - Carseats.test$Sales)^2)
#Test MSE is 3.15
importance(randomForest.carseats)
#The Price and Shelveloc are again the most important variables
importance(randomForest.carseats)
mean((y_pref.rf - Carseats.test$Sales)^2)
source('~/.active-rstudio-document', echo=TRUE)
boot(Portfolio,alpha.fn,R=1000)
summar(lm.fit)
summary(lm.fit)
summary(lm.fit2)
summary(fitlm)
nrow(dataPCA)
train1=dataPCA(10000,)
train1=dataPCA[10000,]
nrow(train1)
train1=dataPCA[1,]
nrow(train1)
train1=dataPCA[1,5]
nrow(train1)
head(dataPCA)
train1=dataPCA[,5]
nrow(train1)
train1=dataPCA
nrow(train1)
train1=dataPCA[1:1000]
train1=dataPCA[PC1,]
train <- sample(1:nrow(Carseats), nrow(Carseats) / 2)
Carseats.train <- Carseats[train, ]
Carseats.test <- Carseats[-train, ]
Carseats.train
nrow(Carseats.train)
class(Carseats)
class(dataPCA)
train1 <- dataPCA[PC1,]
train1 <- dataPCA[1000,]
mrow(train1)
train1 <- dataPCA[1000,]
nrow(train1)
train
train <- sample(1:nrow(dataPCA), nrow(dataPCA) / 2)
train1 <- dataPCA[train,]
nrow(train1)
?sample
train <- sample(1:nrow(dataPCA), nrow(dataPCA) / 1.5)
train1 <- dataPCA[train,]
nrow(train1)
train <- sample(1:nrow(dataPCA), nrow(dataPCA) / 0.8)
train <- sample(1:nrow(dataPCA), nrow(dataPCA) * 0.8)
train1 <- dataPCA[train,]
nrow(train1)
nrow(dataPCA.train)
train <- sample(1:nrow(dataPCA), nrow(dataPCA) * 0.8)
dataPCA.train <- dataPCA[train,]
dataPCA.test <- dataPCA[-train,]
nrow(dataPCA.train)
nrow(dataPCA.test)
set.seed(2)
train=sample(392,196)
lm.fit=lm(mpg~horsepower,subset=train)
mean((mpg-predict(lm.fit,Auto))[-train]^2)
lm.fit2=lm(mpg~poly(horsepower,2),data=Auto,subset=train)
mean((mpg-predict(lm.fit2,Auto))[-train]^2)
lm.fit3=lm(mpg~poly(horsepower,3),data=Auto,subset=train)
mean((mpg-predict(lm.fit3,Auto))[-train]^2)
fit.lm1 <- lm(formula = pm2.5 ~ poly(PC1+PC2+PC3+PC4+cbwd), data=dataPCA)
fit.lm1 <- lm(formula = pm2.5 ~ poly(PC1,PC2,PC3,PC4,cbwd), data=dataPCA)
fit.lm1 <- lm(formula = pm2.5 ~ poly(PC3,PC4,cbwd), data=dataPCA)
head(dataPCA$cbwd)
fit.lm1 <- lm(formula = pm2.5 ~ poly(PC3), data=dataPCA)
summary(fitlm1)
summary(fit.lm1)
fit.lm1 <- lm(formula = pm2.5 ~ poly(PC3,PC4), data=dataPCA)
summary(fit.lm1)
fit.lm1 <- lm(formula = pm2.5 ~ poly(PC1,PC2,PC3,PC4), data=dataPCA)
summary(fit.lm1)
fit.lm1 <- lm( pm2.5 ~PC1^2 + PC2, data=dataPCA)
summary(fit.lm1)
library(caret)
install.packages("caret")
install.packages("caret")
fit.lm1 <- lm( pm2.5 ~PC1^2 + PC2, data=dataPCA)
summary(fit.lm1)
train_index <- createDataPartition(dataPCA,
p = 0.8,
list = FALSE,
times = 10)
library(caret)
train_index <- createDataPartition(dataPCA,
p = 0.8,
list = FALSE,
times = 10)
install.packages("caret")
train_index <- createDataPartition(dataPCA,
p = 0.8,
list = FALSE,
times = 10)
library(caret)
install.packages('caret', dependencies = TRUE)
library(caret)
update.packages('rlang')
library(caret)
